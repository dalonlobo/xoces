[
  {
    "endpage": 6, 
    "keywordlist": [], 
    "level": 1, 
    "startpage": 1, 
    "topicname": "Contents"
  }, 
  {
    "endpage": 7, 
    "keywordlist": [], 
    "level": 1, 
    "startpage": 7, 
    "topicname": "Website"
  }, 
  {
    "endpage": 10, 
    "keywordlist": [], 
    "level": 1, 
    "startpage": 8, 
    "topicname": "Acknowledgments"
  }, 
  {
    "endpage": 14, 
    "keywordlist": [], 
    "level": 1, 
    "startpage": 11, 
    "topicname": "Notation"
  }, 
  {
    "endpage": 42, 
    "keywordlist": [
      "yy yy yy yy h 2h", 
      "Encoder", 
      "Factors of variation", 
      "Weights,", 
      "Neuroscience", 
      "Sigmoid, Sigmoid belief network", 
      "conditions gence", 
      "yy yy x 2x", 
      "x 2x", 
      "Base network h 2h", 
      "Arti\ufb01cial intelligence", 
      "Deep Boltzmann machine,", 
      "h 1h", 
      "Recognition Challenge Challenge", 
      "Machine learning", 
      "Representation learning", 
      "CHAPTER 7. REGULARIZATION FOR DEEP LEARNING smaller. yy h 1h", 
      "Multilayer perceptron", 
      "Convolution, Convolutional network", 
      "Cyc", 
      "yy yy h 1h", 
      "Deep Blue", 
      "h 2h", 
      "Chess", 
      "Parallel distributed processing", 
      "descent descent", 
      "Naive Bayes", 
      "yy yy h 2h", 
      "Ensemble of Sub-Networks yy yy h 2h", 
      "Early stopping, EBM, Echo state network,", 
      "Recti\ufb01ed linear unit, Recurrent network", 
      "Multi-task learning, Multilayer perception", 
      "SML, Softmax, Softplus, Spam detection", 
      "Dataset augmentation, DBM, DCGAN, Decision tree, Decoder", 
      "chine bound Challenge", 
      "Visible layer", 
      "Connectionism,", 
      "Neural network", 
      "x 1x", 
      "Hidden layer,"
    ], 
    "level": 1, 
    "startpage": 15, 
    "topicname": "Chapter 1 - Introduction"
  }, 
  {
    "endpage": 178, 
    "keywordlist": [
      "Eigendecomposition", 
      "Jacobian matrix, Joint probability", 
      "Backprop,", 
      "GPU, Gradient", 
      "Determinant, xii Diagonal matrix", 
      "Linear combination", 
      "Eigenvalue", 
      "Mixture distribution", 
      "Cross-entropy,", 
      "Multinomial distribution", 
      "Bias, Bias parameter", 
      "Convolutional neural network, Coordinate descent, Correlation", 
      "bound mension xi xii", 
      "Lipschitz continuous", 
      "xi xii", 
      "Manifold hypothesis", 
      "Regularization, Regularizer", 
      "Almost sure convergence", 
      "Uniform distribution", 
      "work", 
      "Unit norm", 
      "Supervised \ufb01ne-tuning, Supervised learning", 
      "Machine translation", 
      "Standard error", 
      "Feature", 
      "Nearest neighbor regression", 
      "Contrastive divergence, Convex optimization", 
      "Frequentist probability", 
      "Line search,", 
      "Score matching, Second derivative", 
      "Zero-data learning, tives bound mension xi xii", 
      "Standard error of the mean, Statistic", 
      "Probability mass function estimation", 
      "Dataset", 
      "Regression", 
      "Tensor, Test set", 
      "Trace operator", 
      "Missing inputs", 
      "Active constraint", 
      "Frequentist statistics", 
      "Parameter initialization, Parameter sharing, Parameter tying, Parametric model", 
      "Hyperparameters, Hypothesis space, i.i.d. assumptions, Identity matrix"
    ], 
    "level": 1, 
    "startpage": 43, 
    "topicname": "Applied Math and Machine Learning Basics"
  }, 
  {
    "endpage": 66, 
    "keywordlist": [
      "Eigendecomposition", 
      "Determinant, xii Diagonal matrix", 
      "Linear combination", 
      "Eigenvalue", 
      "bound mension xi xii", 
      "xi xii", 
      "Unit norm", 
      "Zero-data learning, tives bound mension xi xii", 
      "Trace operator", 
      "Dot product,", 
      "Scalar, xi xii", 
      "Eigenvector", 
      "Frobenius norm", 
      "Markov network, Markov random \ufb01eld, Matrix, xi xii", 
      "Dot product,", 
      "xii", 
      "Matrix product", 
      "Boltzmann machine, BPTT, Broadcasting", 
      "Identity and Inverse Matrices", 
      "Multiplying Matrices and Vectors", 
      "Matrix inverse", 
      "Hyperparameters, Hypothesis space, i.i.d. assumptions, Identity matrix", 
      "Linear Dependence and Span", 
      "Linear combination", 
      "Speech recognition, Sphering, Spike and slab restricted Boltzmann ma- SPN, Square matrix", 
      "Linear dependence", 
      "Norms", 
      "xiv", 
      "Triangle inequality", 
      "Error function, ESN, Euclidean norm", 
      "Loss function, Lp norm", 
      "Special Kinds of Matrices and Vectors", 
      "Determinant, xii Diagonal matrix", 
      "Unit norm", 
      "Unit vector", 
      "Orthogonality", 
      "Max norm", 
      "Eigendecomposition", 
      "Eigenvalue", 
      "Eigenvector", 
      "Optimization, Orthodox statistics, Orthogonal matching pursuit, Orthogonal matrix", 
      "Singular Value Decomposition", 
      "The Moore-Penrose Pseudoinverse", 
      "The Trace Operator", 
      "The Determinant"
    ], 
    "level": 2, 
    "startpage": 45, 
    "topicname": "Chapter 2 - Linear Algebra"
  }, 
  {
    "endpage": 111, 
    "keywordlist": [
      "GPU, Gradient", 
      "Lipschitz continuous", 
      "Line search,", 
      "Score matching, Second derivative", 
      "Inequality constraint", 
      "Objective function", 
      "Hill climbing", 
      "Directed graphical model, Directional derivative", 
      "Second derivative test", 
      "Pooling, Positive de\ufb01nite", 
      "Generalized Lagrange function, ized Lagrangian Generalized Lagrangian", 
      "Partial derivative", 
      "Negative de\ufb01nite", 
      "Lipschitz constant", 
      "Equality constraint", 
      "k-means, k-nearest neighbors, Karush-Kuhn-Tucker conditions, Karush\u2013Kuhn\u2013Tucker", 
      "Learning rate", 
      "Active constraint", 
      "Objective function", 
      "Overflow and Underflow", 
      "Gradient-Based Optimization", 
      "GPU, Gradient", 
      "Lipschitz continuous", 
      "Lipschitz constant", 
      "Learning rate"
    ], 
    "level": 2, 
    "startpage": 94, 
    "topicname": "Chapter 4 - Numerical Computation"
  }, 
  {
    "endpage": 178, 
    "keywordlist": [
      "Backprop,", 
      "Bias, Bias parameter", 
      "Manifold hypothesis", 
      "Regularization, Regularizer", 
      "Almost sure convergence", 
      "work", 
      "Supervised \ufb01ne-tuning, Supervised learning", 
      "Machine translation", 
      "Standard error", 
      "Feature", 
      "Nearest neighbor regression", 
      "Contrastive divergence, Convex optimization", 
      "Standard error of the mean, Statistic", 
      "Probability mass function estimation", 
      "Dataset", 
      "Regression", 
      "Tensor, Test set", 
      "Missing inputs", 
      "DAE, Data generating distribution, Data generating process", 
      "Partition function, PCA, PCD, Perceptron, Persistent contrastive divergence, Perturbation analysis, Point estimator", 
      "Transcription", 
      "Training error", 
      "E\ufb00ective capacity", 
      "MAP approximation,", 
      "Non-parametric model", 
      "Manifold learning", 
      "Classi\ufb01cation", 
      "Template matching", 
      "Newton\u2019s method, NLM, NLP, No free lunch theorem", 
      "Derivative, Design matrix", 
      "Unbiased", 
      "Statistical learning theory", 
      "Frequentist statistics", 
      "Parameter initialization, Parameter sharing, Parameter tying, Parametric model"
    ], 
    "level": 2, 
    "startpage": 112, 
    "topicname": "Chapter 5 - Machine Learning Basics"
  }, 
  {
    "endpage": 133, 
    "keywordlist": [
      "Bias, Bias parameter", 
      "Regularization, Regularizer", 
      "Nearest neighbor regression", 
      "Tensor, Test set", 
      "DAE, Data generating distribution, Data generating process", 
      "Training error", 
      "E\ufb00ective capacity", 
      "Adversarial training, A\ufb03ne", 
      "Representational capacity", 
      "Generalization", 
      "Batch normalization, Bayes error", 
      "VAE, Vapnik-Chervonenkis dimension", 
      "Non-parametric model", 
      "Newton\u2019s method, NLM, NLP, No free lunch theorem", 
      "Statistical learning theory", 
      "Parameter initialization, Parameter sharing, Parameter tying, Parametric model"
    ], 
    "level": 3, 
    "startpage": 124, 
    "topicname": "5.2 Capacity, Overfitting and Underfitting"
  }, 
  {
    "endpage": 241, 
    "keywordlist": [
      "Universal approximation theorem", 
      "Chain rule (calculus)", 
      "Layer (neural network)", 
      "KKT, KKT conditions, KL divergence, Knowledge base, Krylov methods", 
      "LCN, Leaky ReLU", 
      "Output layer", 
      "Computational graph", 
      "Forward propagation", 
      "Hessian matrix, Heteroscedastic", 
      "RBF", 
      "Activation function", 
      "conditions gence Knowledge base, Krylov methods", 
      "tanh", 
      "position", 
      "Parametric ReLU", 
      "Operation", 
      "CAE, Calculus of variations", 
      "Radial basis function", 
      "Feedforward neural network", 
      "Mixture density networks", 
      "Helmholtz free energy, Hessian", 
      "Back-propagation", 
      "0-1 loss, Absolute value recti\ufb01cation"
    ], 
    "level": 2, 
    "startpage": 181, 
    "topicname": "Chapter 6 - Deep Feedforward Networks"
  }, 
  {
    "endpage": 343, 
    "keywordlist": [
      "Forget gate", 
      "Transpose, tion trick Stochastic gradient descent,", 
      "Surrogate loss function", 
      "Initialization", 
      "Norm, Normal distribution, Normal equations, Normalized initialization", 
      "Quasi-Newton methods", 
      "Greedy layer-wise unsupervised pretraining, Greedy supervised pretraining", 
      "Empirical risk", 
      "Saddle points", 
      "Condition number", 
      "CTC, Curriculum learning", 
      "Model identi\ufb01ability", 
      "Continuation methods", 
      "Negative phase, Neocognitron, Nesterov momentum", 
      "Ridge regression, Risk", 
      "AdaGrad", 
      "BFGS", 
      "Fine-tuning", 
      "Empirical risk minimization", 
      ",xiv", 
      "Wake-sleep, Weight decay, Weight space symmetry", 
      "Greedy algorithm", 
      "Minibatch"
    ], 
    "level": 2, 
    "startpage": 288, 
    "topicname": "Chapter 8 - Optimization for Training Deep Models"
  }, 
  {
    "endpage": 386, 
    "keywordlist": [
      "Detector layer", 
      "Receptive \ufb01eld", 
      "ences trick", 
      "Simple cell", 
      "Volumetric data", 
      "Complex cell", 
      "Time-delay neural network, Toeplitz matrix", 
      "Tikhonov regularization, Tiled convolution", 
      "Unsupervised learning, Unsupervised pretraining, V-structure, V1", 
      "FVBN, Gabor function", 
      "Pretraining, Primary visual cortex", 
      "Integral, xiii Invariance", 
      "Doubly block circulant matrix", 
      "Quadrature pair", 
      "of probability Quadrature pair", 
      "Separable convolution", 
      "Fourier transform, Fovea", 
      "Restricted Boltzmann machine,", 
      "Cross-correlation", 
      "Equivariance", 
      "tic maximum likelihood", 
      "Max pooling", 
      "Collider, Color images"
    ], 
    "level": 2, 
    "startpage": 344, 
    "topicname": "Chapter 9 - Convolutional Networks"
  }, 
  {
    "endpage": 501, 
    "keywordlist": [
      "Audio, Autoencoder, Automatic speech recognition", 
      "Word-sense disambiguation", 
      "Triangulated graph, Trigram", 
      "Contextual bandits", 
      "Clique potential, CNN, Collaborative Filtering", 
      "Gradient clipping, Gradient descent, Graph, xii Graphical model, Graphics processing unit", 
      "WordNet", 
      "Policy", 
      "Model compression", 
      "Wikibase", 
      "Word embedding", 
      "Numerical di\ufb00erentiation, Object detection", 
      "Local conditional probability distribution, Local contrast normalization", 
      "Contrast", 
      "Connectionist temporal classi\ufb01cation", 
      "Recommender Systems", 
      "Exploitation", 
      "Whitening", 
      "Content-based recommender systems", 
      "Class-based language models", 
      "Gaussian mixture, GCN, GeneOntology", 
      "n-gram", 
      "Bag of words", 
      "Linear regression, Link prediction", 
      "Bigram", 
      "Free energy, Freebase", 
      "Natural language processing", 
      "Computer vision", 
      "Net\ufb02ix Grand Prize, Neural language model,", 
      "Binary relation", 
      "Object recognition", 
      "Unigram", 
      "Data parallelism", 
      "Gibbs sampling, Global contrast normalization", 
      "Set, xii SGD, Shannon entropy, Shortlist", 
      "Exploration", 
      "Reinforcement learning, Relational database", 
      "Model parallelism", 
      "Preprocessing"
    ], 
    "level": 2, 
    "startpage": 459, 
    "topicname": "Chapter 12 - Applications"
  }, 
  {
    "endpage": 605, 
    "keywordlist": [
      "Hadamard product, Hard Harmonium, Harmony theory", 
      "Boltzmann distribution", 
      "Ancestral sampling,", 
      "Chordal graph", 
      "Factor (graphical model)", 
      "Energy function", 
      "Undirected graphical model, Undirected model", 
      "Loopy belief propagation", 
      "Product of experts", 
      "D-separation", 
      "ized Lagrangian", 
      "Gibbs distribution", 
      "Structure learning", 
      "Factor graph", 
      "Separation (probabilistic modeling)", 
      "Approximate inference", 
      "ILSVRC, ImageNet Large-Scale Visual Recognition Immorality", 
      "Logistic regression, Logistic sigmoid, Long short-term memory, Loop", 
      "Context-speci\ufb01c independence", 
      "Chord", 
      "Natural image", 
      "Expected value, Explaining away,", 
      "Moore-Penrose pseudoinverse, Moralized graph", 
      "Unnormalized probability distribution"
    ], 
    "level": 2, 
    "startpage": 574, 
    "topicname": "Chapter 16 - Structured Probabilistic Models for Deep Learning"
  }, 
  {
    "endpage": 736, 
    "keywordlist": [
      "recognition chine", 
      "Lagrange multipliers, Lagrangian, LAPGAN", 
      "Discriminative \ufb01ne-tuning, Discriminative RBM", 
      "ANN, Annealed importance sampling, Approximate Bayesian computation", 
      "RNN-RBM", 
      "NADE", 
      "Multi-prediction DBM", 
      "Generative adversarial networks, Generative moment matching networks", 
      "Moment matching", 
      "Categorical distribution, CD, Centering trick (DBM)", 
      "REINFORCE", 
      "Probabilistic max pooling", 
      "\ufb01ne-tuning Discriminative RBM", 
      "Importance sampling, Importance weighted autoencoder", 
      "Conditional RBM", 
      "Reparametrization trick", 
      "Fully-visible Bayes network", 
      "Generator network"
    ], 
    "level": 2, 
    "startpage": 670, 
    "topicname": "Chapter 20 - Deep Generative Models"
  }
]

